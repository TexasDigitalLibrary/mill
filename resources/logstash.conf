input {
  # See: http://logstash.net/docs/1.4.1/inputs/lumberjack
  lumberjack {

    # Group stack traces lines that span multiple lines into a single event
    # See: http://logstash.net/docs/1.4.1/codecs/multiline
    codec => multiline {
      pattern => "(^\s)|(^Caused by:)|(^.+Exception: .+)"
      what => "previous"
    }

    port => 5555
    type => notype
    ssl_certificate => "/opt/logstash/logstash.crt"
    ssl_key => "/opt/logstash/logstash.key"
  }
}

filter {
  # If the event has a field named "source" whose value contains the substring "dc-mill"
  # then apply the contained filter inside the if conditional statement.
  if "dc-mill" in [source] {
    # See: http://logstash.net/docs/1.4.1/filters/grok
    grok {
      match => [
        # There are 3 different log statements that are important for us to parse values
        # out of.  Each message pattern below will try to be matched until one is found.
        "message", "dup_size=%{INT:dup_size:int}%{SPACE}space=%{NOTSPACE}%{SPACE}account=%{WORD:account}",
        "message", "dup_lp_qsize=%{INT:dup_lp_qsize:int} dup_hp_qsize=%{INT:dup_hp_qsize:int} dup_dl_qsize=%{INT:dup_dl_qsize:int}",
        "message", "Session stats \(global incremental\): dups=%{INT:ltp_dups:int} deletes=%{INT:ltp_deletes:int}"
      ]
    }
  }

  # The next 3 blocks (grok, date, mutate) attempt to pull a timestamp from the message
  # and set it as the timestamp within ElasticSearch.  This filter is applied to all
  # events since no conditional is used.
  # By default LogStash assigns a timestamp to all events based on when LogStash received
  # the event.  This field is "@timestamp" which is the same field ElasticSearch uses
  # for time based searches.  It is much more useful to use a timestamp that is part
  # of the log event.  In order to do this we must attempt to parse/grok out a timestamp
  # which we assign to a new field "log_timestamp".  If a match is found for one of
  # the timestamp patterns then we assign the current value in "@timestamp" to a new
  # field called "received_at".
  # The date filter then says if there is a value in the "log_timestamp" field and it
  # matches one of the supplied patterns then is will be assigned to the "@timestamp" field.
  # Last, remove the temporary field we added called "log_timestamp".
  grok {
    match => [
      "message", "%{YMDTIME:log_timestamp}",
      "message", "%{TIMESTAMP_ISO8601:log_timestamp}"
    ]
    patterns_dir => "/opt/logstash/patterns"
    add_field => [ "received_at", "%{@timestamp}" ]
  }
  # See: http://logstash.net/docs/1.4.1/filters/date
  date {
    match => [ "log_timestamp", "yyyy/MM/dd HH:mm:ss", "yyyy-MM-dd HH:mm:ss", "yyyy-MM-dd HH:mm:ss,SSS", "ISO8601" ]
  }
  mutate {
    remove_field => [ "log_timestamp" ]
  }

}

# Send the data to ElasticSearch which is running on the same machine as LogStash
output {
  elasticsearch {
    host => "127.0.0.1"
  }
}